# Подключаем Google Drive
from google.colab import drive
drive.mount('/content/drive')


# Импортируем необходимые библиотеки
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np

# Указание путей к данным на Google Drive
train_dir = '/content/drive/MyDrive/dino-dragon/train'
test_dir = '/content/drive/MyDrive/dino-dragon/test'

# Параметры для генераторов изображений
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Создание генераторов изображений
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')
# Создание модели CNN
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Компиляция модели с SGD оптимизатором и бинарной кросс-энтропией
model.compile(optimizer=SGD(learning_rate=0.002, momentum=0.8), # Обратите внимание на изменение здесь
              loss='binary_crossentropy',
              metrics=['accuracy'])
# Обучение модели
history = model.fit(
    train_generator,
    epochs=10,
    validation_data=test_generator)


model.summary()

# Вывод медианы точности и стандартного отклонения потерь для ответа на вопросы 3 и 4
median_accuracy = np.median(history.history['accuracy'])
std_loss = np.std(history.history['loss'])
print(f"Медиана точности обучения: {median_accuracy:.2f}")
print(f"Стандартное отклонение потерь: {std_loss:.2f}")

# Добавление аугментации данных
train_datagen_augmented = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

train_generator_augmented = train_datagen_augmented.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')


# Обучение модели с аугментированными данными
history_augmented = model.fit(
    train_generator_augmented,
    epochs=10,
    validation_data=test_generator)

# Вывод результатов
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

print("Общее количество параметров в модели:", model.count_params())
# Расчет среднего значения потерь и точности на тестовом наборе данных после аугментации для вопросов 5 и 6
avg_loss_after_augmentation = np.mean(history_augmented.history['val_loss'])
avg_accuracy_last_5_epochs = np.mean(history_augmented.history['val_accuracy'][-5:])
print(f"Среднее значение потерь после аугментации: {avg_loss_after_augmentation:.2f}")
print(f"Средняя точность за последние 5 эпох: {avg_accuracy_last_5_epochs:.2f}")
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt


def upload_and_predict():
    uploaded = files.upload()  

    for fn in uploaded.keys():
      
        path = '/content/' + fn
        img = image.load_img(path, target_size=(150, 150))
        
        x = image.img_to_array(img)
        x /= 255
        x = np.expand_dims(x, axis=0)
        
        images = np.vstack([x])
        classes = model.predict(images, batch_size=10)
        
        plt.imshow(img)
        plt.show()

       
        if classes[0]>0.5:
            print(f"{fn} is a dragon")
        else:
            print(f"{fn} is a dino")


upload_and_predict()
